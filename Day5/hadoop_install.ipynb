{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hadoop_install.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNzJCDbblLt/1SgwmYCB+3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPIGP-mwKBD",
        "colab_type": "text"
      },
      "source": [
        "#Hadoop Instalation Part\n",
        "Hadoop is a Java-based programming framework that supports the processing and storage of extremely large datasets on a cluster of inexpensive machines. It was the first major open source project in the big data playing field and is sponsored by the Apache Software Foundation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bT9M1yvyXG",
        "colab_type": "text"
      },
      "source": [
        "## Step 1:Installing Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bijZAdD_cBMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "989d7a35-005b-4014-a0cf-ff458ddd46d7"
      },
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-10 05:21:56--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 2a01:4f8:10a:201a::2\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500749234 (478M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.0.tar.gz.1’\n",
            "\n",
            "hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  10.4MB/s    in 47s     \n",
            "\n",
            "2020-09-10 05:22:44 (10.1 MB/s) - ‘hadoop-3.3.0.tar.gz.1’ saved [500749234/500749234]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mj40txsTw6DZ"
      },
      "source": [
        "we’ll use the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVce513-cBHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzvf hadoop-3.3.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF-ze-YOdync",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy  hadoop file to user/local\n",
        "!cp -r hadoop-3.3.0/ /usr/local/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh6Dqbbrwqpe",
        "colab_type": "text"
      },
      "source": [
        "## Step2:Configuring Hadoop’s Java Home\n",
        "Hadoop requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OUc19ZtcBG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3821595-1aa0-4f19-a6df-05c7f74c1ae1"
      },
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxaPBWRKxXta",
        "colab_type": "text"
      },
      "source": [
        "To set java path, go to /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh then\n",
        "\n",
        ". . .\n",
        "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\n",
        " . . . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj00rPPZyEWZ",
        "colab_type": "text"
      },
      "source": [
        "# Step 3:  Running Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhf-zK7NcBDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "4b567ac3-57ad-4992-9e1d-e5084ae61b46"
      },
      "source": [
        "#Running Hadoop\n",
        "!/usr/local/hadoop-3.3.0/bin/hadoop"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI-YBPIzcBCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ~/input\n",
        "!cp /usr/local/hadoop-3.3.0/etc/hadoop/*.xml ~/input"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DuDJIsPcA98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d2e82548-75aa-4e40-a048-48928825d374"
      },
      "source": [
        "!ls ~/input"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "capacity-scheduler.xml\thdfs-rbf-site.xml  kms-acls.xml     yarn-site.xml\n",
            "core-site.xml\t\thdfs-site.xml\t   kms-site.xml\n",
            "hadoop-policy.xml\thttpfs-site.xml    mapred-site.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZi5zOGKyySH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep ~/input ~/grep_example 'allowed[.]*'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtr0xWbfcA5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8debaca6-e5f5-423f-e85f-0b3f0753d3e8"
      },
      "source": [
        "!cat ~/grep_example/*"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\tallowed.\n",
            "1\tallowed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}